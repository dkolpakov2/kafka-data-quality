{
  "name": "Flink SQL Data Quality – Cassandra Cloud",
  "paragraphs": [
    {
      "title": "01 Kafka Source (Cloud)",
      "text": "%flink.sql\nCREATE TABLE kafka_cloud_source (\n  id STRING,\n  event_ts TIMESTAMP(3),\n  value DOUBLE,\n  raw_json STRING,\n  WATERMARK FOR event_ts AS event_ts - INTERVAL '5' SECOND\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'topic_cloud_sales',\n  'properties.bootstrap.servers' = 'kafka:9092',\n  'format' = 'json'\n);"
    },
    {
      "title": "02 DQ Enrichment + Canonical Hash",
      "text": "%flink.sql\nCREATE VIEW dq_enriched AS\nSELECT\n  id,\n  event_ts,\n  value,\n  SHA256(\n    CONCAT_WS('|',\n      COALESCE(id, 'NULL'),\n      COALESCE(CAST(value AS STRING), 'NULL'),\n      CAST(event_ts AS STRING)\n    )\n  ) AS record_hash,\n  raw_json\nFROM kafka_cloud_source;"
    },
    {
      "title": "03 Data Quality Rules",
      "text": "%flink.sql\nCREATE VIEW dq_rules AS\nSELECT *,\n  CASE\n    WHEN id IS NULL THEN 'FAIL_ID_NULL'\n    WHEN value < 0 THEN 'FAIL_NEG_VALUE'\n    WHEN event_ts IS NULL THEN 'FAIL_TS_NULL'\n    ELSE 'PASS'\n  END AS dq_status\nFROM dq_enriched;"
    },
    {
      "title": "04 Cassandra Sink (Azure Cloud)",
      "text": "%flink.sql\nCREATE TABLE cassandra_cloud_sink (\n  id STRING,\n  event_ts TIMESTAMP(3),\n  value DOUBLE,\n  record_hash STRING,\n  PRIMARY KEY (id) NOT ENFORCED\n) WITH (\n  'connector' = 'cassandra',\n  'keyspace' = 'sales',\n  'table' = 'events',\n  'hosts' = 'cassandra-cloud.internal',\n  'datacenter' = 'DC_AZURE'\n);"
    },
    {
      "title": "05 Insert Valid Records",
      "text": "%flink.sql\nINSERT INTO cassandra_cloud_sink\nSELECT id, event_ts, value, record_hash\nFROM dq_rules\nWHERE dq_status = 'PASS';"
    },
    {
      "title": "06 DLQ – Invalid Records",
      "text": "%flink.sql\nCREATE TABLE dq_dlq_cloud (\n  id STRING,\n  event_ts TIMESTAMP(3),\n  value DOUBLE,\n  record_hash STRING,\n  dq_status STRING,\n  raw_json STRING\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'dq_dlq_cloud',\n  'properties.bootstrap.servers' = 'kafka:9092',\n  'format' = 'json'\n);\n\nINSERT INTO dq_dlq_cloud\nSELECT id, event_ts, value, record_hash, dq_status, raw_json\nFROM dq_rules\nWHERE dq_status <> 'PASS';"
    },
    {
      "title": "07 Valid Row Counter",
      "text": "%flink.sql\nCREATE TABLE dq_valid_metrics (\n  job_name STRING,\n  metric_ts TIMESTAMP(3),\n  valid_count BIGINT\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'dq_metrics_valid',\n  'properties.bootstrap.servers' = 'kafka:9092',\n  'format' = 'json'\n);\n\nINSERT INTO dq_valid_metrics\nSELECT\n  'flink_cloud_job' AS job_name,\n  TUMBLE_END(event_ts, INTERVAL '1' MINUTE) AS metric_ts,\n  COUNT(*) AS valid_count\nFROM dq_rules\nWHERE dq_status = 'PASS'\nGROUP BY TUMBLE(event_ts, INTERVAL '1' MINUTE);"
    },
    {
      "title": "08 Invalid Row Counter",
      "text": "%flink.sql\nCREATE TABLE dq_invalid_metrics (\n  job_name STRING,\n  metric_ts TIMESTAMP(3),\n  invalid_count BIGINT\n) WITH (\n  'connector' = 'kafka',\n  'topic' = 'dq_metrics_invalid',\n  'properties.bootstrap.servers' = 'kafka:9092',\n  'format' = 'json'\n);\n\nINSERT INTO dq_invalid_metrics\nSELECT\n  'flink_cloud_job' AS job_name,\n  TUMBLE_END(event_ts, INTERVAL '1' MINUTE) AS metric_ts,\n  COUNT(*) AS invalid_count\nFROM dq_rules\nWHERE dq_status <> 'PASS'\nGROUP BY TUMBLE(event_ts, INTERVAL '1' MINUTE);"
    }
  ]
}